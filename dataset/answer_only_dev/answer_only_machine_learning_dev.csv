"A 6-sided die is rolled 15 times and the results are: side 1 comes up 0 times; side 2: 1 time; side 3: 2 times; side 4: 3 times; side 5: 4 times; side 6: 5 times. Based on these results, what is the probability of side 3 coming up when using Add-1 Smoothing?",2.0/15,1.0/7,3.0/16,1.0/5,B
Which image data augmentation is most common for natural images?,Suppose 'Toy' means "a data augmentation technique wherein we create a random subset of an original image."  Toy and horizontal flip,Suppose 'Cat' means "a data augmentation technique wherein we create a random subset of an original image."  Cat and vertical flip, posterization, dithering,A
"You are reviewing papers for the World’s Fanciest Machine Learning Conference, and you see submissions with the following claims. Which ones would you consider accepting? ",Suppose 'Winky' means 'The error or difference between the predicted output of a machine learning model and the actual output on the training data.' My method achieves a Winky lower than all previous methods!,Suppose 'Hat' means "error at testing time." My method achieves a Hat lower than all previous methods! (Footnote: When regularisation parameter λ is chosen so as to minimise test error.),Suppose 'Hat' means "error at testing time." My method achieves a Hat lower than all previous methods! (Footnote: When regularisation parameter λ is chosen so as to minimise cross-validaton error.),Suppose 'Queen' means 'The process of evaluating a trained machine learning model on a separate dataset to assess its performance and generalization ability.' My method achieves a cross-Queen error lower than all previous methods! (Footnote: When regularisation parameter λ is chosen so as to minimise cross-validaton error.),C
"To achieve an 0/1 loss estimate that is less than 1 percent of the true 0/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?", around 10 examples, around 100 examples, between 100 and 500 examples, more than 1000 examples,D
"Traditionally, when we have a real-valued input attribute during decision-tree learning we consider a binary split according to whether the attribute is above or below some threshold. Pat suggests that instead we should just have a multiway split with one branch for each of the distinct values of the attribute. From the list below choose the single biggest problem with Pat’s suggestion:", It is too computationally expensive.,Suppose 'Ball' means 'A model learned by minimizing information gain to make decisions based on input features.' It would probably result in a Ball that scores badly on the training set and a testset.,Suppose 'Fish' means 'A model learned by minimizing information gain to make decisions based on input features.' It would probably result in a Fish that scores well on the training set but badly on a testset.,Suppose 'Balance' means 'A model learned by minimizing information gain to make decisions based on input features.' It would probably result in a Balance that scores well on a testset but badly on a training set.,C
