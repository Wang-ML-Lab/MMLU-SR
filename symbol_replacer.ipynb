{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "import string\n",
        "from collections import Counter\n",
        "import re\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "lTc_nhL4iv9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "w_ZuqBVYMKUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract key terms"
      ],
      "metadata": {
        "id": "UmtRz6vN5EEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY=''"
      ],
      "metadata": {
        "id": "E_H_n1I_05o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def read_input_file_by_chunks(file_path, chunk_size=20):\n",
        "    \"\"\"Yields chunks of the input file, with each chunk containing up to 'chunk_size' rows.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        chunk = []\n",
        "        for line in file:\n",
        "            chunk.append(line.strip())\n",
        "            if len(chunk) == chunk_size:\n",
        "                yield \" \".join(chunk)\n",
        "                chunk = []\n",
        "        if chunk:  # Handle any remaining lines\n",
        "            yield \" \".join(chunk)\n",
        "\n",
        "def process_chunk_and_extract_terms(chunk, subject_context):\n",
        "    \"\"\"Extracts key terms and their definitions from the input text and returns them in JSON format.\"\"\"\n",
        "    # Few-shot examples to guide the model\n",
        "    few_shot_examples = {\n",
        "        \"chemistry\": {\n",
        "            \"MHz\": \"Megahertz, a unit of frequency equal to one million hertz.\",\n",
        "            \"compounds\": \"a substance made up of two or more different chemical elements combined in a fixed ratio.\"\n",
        "        },\n",
        "        \"physics\": {\n",
        "            \"speed\": \"The rate at which someone or something is able to move or operate.\",\n",
        "            \"Hz\": \"Hertz, the unit of frequency in the International System of Units, representing one cycle per second.\"\n",
        "        },\n",
        "        \"computer science\": {\n",
        "            \"memory-mapped I/O\": \"A method for performing I/O between the CPU and peripheral devices using memory addresses\",\n",
        "            \"MD5\": \"Message Digest Algorithm 5, a widely used cryptographic hash function that produces a 128-bit hash value.\"\n",
        "        },\n",
        "        \"history\": {\n",
        "            \"Louis XIV\": \"King of France known for his long reign and his centralized control of the government and the economy\",\n",
        "            \"Great Awakening\": \"a series of religious revivals in American colonies during the 18th and 19th centuries\"\n",
        "        },\n",
        "        \"economics\": {\n",
        "            \"MP\": \"marginal product, the additional output produced by using one more unit of input\",\n",
        "            \"marginal product of labor\": \"The change in output resulting from employing one more unit of labor.\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    examples = few_shot_examples.get(subject_context, {})\n",
        "    system_message = \"As an expert glossary generator, here are some examples of key terms and their definitions:\\n\"\n",
        "    for term, definition in examples.items():\n",
        "        system_message += f'\"{term}\": \"{definition}\",\\n'\n",
        "\n",
        "    system_message += (f\"Now, for {subject_context}, extract key terms from the provided text, ensuring each term has a unique and universally applicable definition within the field. \"\n",
        "                       \"Terms should be extracted exactly as they appear, without alteration. Focus on significant concepts or units within the {subject_context} context, \"\n",
        "                       \"providing clear, informative, and comprehensive definitions. Avoid repetition of terms with varying definitions; each term should have one definitive explanation. \"\n",
        "                       \"Ignore overly simple or generic terms unless they are of specific relevance. Output should be in JSON format with terms as keys and their concise yet complete definitions as values.\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.5,\n",
        "        max_tokens=4096,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": chunk}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "def extract_terms_and_definitions(input_file_path, subject_context):\n",
        "    \"\"\"Extracts key terms and their definitions from the input file, processing in chunks.\"\"\"\n",
        "    for chunk in read_input_file_by_chunks(input_file_path):\n",
        "        process_chunk_and_extract_terms(chunk, subject_context)\n",
        "\n",
        "# Example usage:\n",
        "input_file_path = 'virology.txt'\n",
        "subject_context = \"virology\"\n",
        "\n",
        "extract_terms_and_definitions(input_file_path, subject_context)"
      ],
      "metadata": {
        "id": "Si6yJOhl5Dht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove duplicate terms in JSON"
      ],
      "metadata": {
        "id": "wZniDSs7uUrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_keys(json_file):\n",
        "    # Read the original JSON data\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Create a new dictionary to store unique keys and values\n",
        "    new_data = {}\n",
        "\n",
        "    # Loop through the original data, add only unique keys to the new dictionary\n",
        "    for key, value in data.items():\n",
        "        if key not in new_data:\n",
        "            new_data[key] = value\n",
        "\n",
        "    # Write the new data with unique keys back to the JSON file\n",
        "    with open(json_file, 'w') as file:\n",
        "        json.dump(new_data, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "# Replace 'your_json_file.json' with the actual JSON file path\n",
        "remove_duplicate_keys('virology_glossary.json')"
      ],
      "metadata": {
        "id": "C8eC-oT0uX41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace word using glossary for questions"
      ],
      "metadata": {
        "id": "VBjB6-xFyCkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glossary(file_path):\n",
        "    \"\"\"Load and return the glossary from a JSON file.\"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def random_word():\n",
        "    \"\"\"Select a random meaningful word.\"\"\"\n",
        "    words = [\"Cat\", \"Fish\", \"Vex\", \"Point\", \"Bard\", \"Book\", \"Dummy\", \"Dog\",\n",
        "             \"Balance\", \"Adam\", \"Winkle\", \"Winky\", \"Noise\", \"Zelly\", \"Luck\", \"Jump\", \"Love\", \"King\", \"Queen\", \"Jack\", \"Ball\",\"Duck\"]\n",
        "    return random.choice(words)\n",
        "\n",
        "def escape_regex_term(term):\n",
        "    \"\"\"Escape special characters in a term for regex use.\"\"\"\n",
        "    return re.escape(term)\n",
        "\n",
        "def replace_terms_in_question(question, glossary):\n",
        "    # Convert the question to lower case for comparison\n",
        "    lower_case_question = question.lower()\n",
        "\n",
        "    # Sort glossary keys by length in descending order to match longer terms first\n",
        "    sorted_terms = sorted(glossary.keys(), key=len, reverse=True)\n",
        "\n",
        "    # Dictionary to store dummy words and their definitions\n",
        "    replaced_terms = {}\n",
        "\n",
        "    for term in sorted_terms:\n",
        "        lower_case_term = term.lower()\n",
        "        if lower_case_term in lower_case_question:\n",
        "            dummy_word = random_word()\n",
        "            definition = glossary[term]\n",
        "            print(f\"Term: {term}, Dummy Word: {dummy_word}, Definition: {definition}\")  # Debugging print statement\n",
        "\n",
        "            # Updated regular expression for term replacement with case-insensitive matching\n",
        "            term_pattern = r'\\b' + re.escape(lower_case_term) + r'\\b'\n",
        "            question, count = re.subn(term_pattern, dummy_word, question, flags=re.IGNORECASE)\n",
        "\n",
        "            # Only add to replaced_terms if actually replaced\n",
        "            if count > 0:\n",
        "                replaced_terms[dummy_word] = definition\n",
        "\n",
        "    # Construct the definition introduction sentence\n",
        "    definition_intro = ' '.join([\n",
        "        \"Suppose '{}' refers to {}\".format(dummy, definition.strip('\"'))\n",
        "        if definition.startswith('\"')\n",
        "        else \"Suppose '{}' means '{}'\".format(dummy, definition)\n",
        "        for dummy, definition in replaced_terms.items()\n",
        "    ])\n",
        "\n",
        "    return definition_intro + ' ' + question\n",
        "\n",
        "\n",
        "    # Return the modified question with the definitions introduced at the beginning\n",
        "    return definition_intro + ' ' + question\n",
        "\n",
        "def process_csv_questions(file_path, output_file_path, glossary_file_path):\n",
        "    # Load the glossary\n",
        "    glossary = load_glossary(glossary_file_path)\n",
        "\n",
        "    # Load CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Replace questions in the first column with revised questions\n",
        "    df.iloc[:, 0] = df.iloc[:, 0].apply(lambda question: replace_terms_in_question(question, glossary))\n",
        "\n",
        "    # Save the DataFrame with revised questions to a new CSV file\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Example usage\n",
        "input_file_path = 'college_biology_test.csv'\n",
        "output_file_path = 'question_only_college_biology_test_easy.csv'\n",
        "glossary_file_path = 'biology_glossary.json'\n",
        "process_csv_questions(input_file_path, output_file_path, glossary_file_path)\n"
      ],
      "metadata": {
        "id": "XIRuXbZLyBUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace answers"
      ],
      "metadata": {
        "id": "QuiCgBQnHXLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def load_glossary(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def random_word():\n",
        "    words = [\"Cat\", \"Fish\", \"Vex\", \"Point\", \"Bard\", \"Book\", \"Dummy\", \"Dog\",\n",
        "             \"Balance\", \"Adam\", \"Winkle\", \"Winky\", \"Noise\", \"Zelly\", \"Luck\", \"Jump\", \"Love\", \"King\", \"Queen\", \"Jack\", \"Ball\",\"Duck\"]\n",
        "    return random.choice(words)\n",
        "\n",
        "def escape_regex_term(term):\n",
        "    return re.escape(term)\n",
        "\n",
        "def replace_terms_in_text(text, glossary):\n",
        "    # Convert text to string to avoid AttributeError when calling .lower()\n",
        "    text = str(text)\n",
        "    lower_case_text = text.lower()\n",
        "    sorted_terms = sorted(glossary.keys(), key=len, reverse=True)\n",
        "    replaced_terms = {}\n",
        "\n",
        "    for term in sorted_terms:\n",
        "        lower_case_term = term.lower()\n",
        "        if lower_case_term in lower_case_text:\n",
        "            dummy_word = random_word()\n",
        "            definition = glossary[term]\n",
        "            term_pattern = r'\\b' + re.escape(lower_case_term) + r'\\b'\n",
        "            text, count = re.subn(term_pattern, dummy_word, text, flags=re.IGNORECASE)\n",
        "            if count > 0:\n",
        "                replaced_terms[dummy_word] = definition\n",
        "\n",
        "    definition_intro = ' '.join([\n",
        "        \"Suppose '{}' refers to {}\".format(dummy, definition.strip('\"'))\n",
        "        if definition.startswith('\"')\n",
        "        else \"Suppose '{}' means '{}'\".format(dummy, definition)\n",
        "        for dummy, definition in replaced_terms.items()\n",
        "    ])\n",
        "\n",
        "    return definition_intro + ' ' + text\n",
        "\n",
        "def process_csv_questions(file_path, output_file_path, glossary_file_path):\n",
        "    glossary = load_glossary(glossary_file_path)\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Replace terms in the answer columns (assumed to be columns 2 to 5)\n",
        "    for column in range(1, 5):\n",
        "        df.iloc[:, column] = df.iloc[:, column].apply(lambda text: replace_terms_in_text(text, glossary))\n",
        "\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Example usage\n",
        "input_file_path = 'college_medicine_test.csv'\n",
        "output_file_path = 'answer_only_college_medicine_test.csv'\n",
        "glossary_file_path = 'medicine_glossary.json'\n",
        "process_csv_questions(input_file_path, output_file_path, glossary_file_path)\n"
      ],
      "metadata": {
        "id": "nhqkjRg1HW4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}