{
    "Linear regression estimator": "A statistical method that estimates the relationship between independent and dependent variables with the smallest variance among unbiased estimators.",
    "AdaBoost": "An ensemble learning method that creates a strong classifier from multiple weak classifiers.",
    "RoBERTa": "A transformer-based language model that pretrains on a corpus approximately 10 times larger than BERT.",
    "ResNeXt": "A neural network architecture that typically used tanh activation functions in 2018.",
	"high-resolution images":"an image that is typically 300 DPI (dots per inch) or higher.",
	"infinity":"the state or quality of being infinite.",
	"random crop":"a data augmentation technique wherein we create a random subset of an original image.",
	"PyTorch":"a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.",
	"events":"an outcome or defined collection of outcomes of a random experiment.",
	"frequentists":"one who defines the probability of an event (such as heads in flipping a coin) as the limiting value of its frequency in a large number of trials compare bayesian.",
	"clusters":"a group of similar things or people positioned or occurring closely together.",
	"AI":"the intelligence of machines or software, as opposed to the intelligence of living beings, primarily of humans.",
	"null space":"the linear subspace of the domain of the map which is mapped to the zero vector.",
	"linear model":"any model which assumes linearity in the system.",
	"region":"an area or division, especially part of a country or the world having definable characteristics but not always fixed boundaries.",
	"rank":"the maximum number of its linearly independent column vectors (or row vectors).",
	"Adam optimizer":"Adaptive Moment Estimation, an iterative optimization algorithm used to minimize the loss function during the training of neural networks.",
	"training data":"the data you use to train an algorithm or machine learning model to predict the outcome you design your model to predict.",
    "Support vector machines": "Models that, like logistic regression, provide a probability distribution over possible labels given an input example.",
    "Support vectors": "Data points that remain consistent across different kernels, such as linear and polynomial, in support vector machines.",
    "Machine learning problem": "A task involving attributes and a class, with a specific number of values for each attribute and class, leading to a finite number of possible examples.",
    "High-resolution images classification": "The best architecture for classifying high-resolution images as of 2020.",
    "Log-likelihood": "The logarithm of the likelihood function, which increases through iterations of the expectation-maximization algorithm.",
    "Q-learning": "A reinforcement learning technique that requires prior knowledge of how actions impact the environment.",
    "Gradient descent update cost": "The cost associated with updating a model's parameters using the gradient of the cost function.",
    "Continuous random variable": "A variable with a probability distribution function that ensures probabilities are between 0 and 1 for all possible values.",
    "Decision tree": "A model learned by minimizing information gain to make decisions based on input features.",
    "Bayesian network parameters": "The number of independent parameters required for a specific Bayesian Network structure.",
    "VC-dimension": "The capacity of a classifier to shatter points in a dataset, with infinite VC-dimension for k-Nearest Neighbour when k=1.",
    "Underfitting": "When a model is too simple to capture the underlying patterns in the data.",
    "F1 score": "A metric useful for evaluating models on imbalanced datasets.",
    "ROC curve": "A graphical representation used to assess anomaly detectors, with the area under the curve as a key metric.",
    "Back-propagation algorithm": "An algorithm that optimizes neural networks with hidden layers to learn globally optimal parameters.",
    "VC dimension of a line": "The maximum number of points a line can shatter, with at most 2 for a line.",
    "High entropy in classification": "Indicative of partitions in classification that are diverse and have high uncertainty.",
    "Layer Normalization": "A normalization technique used in the original ResNet paper instead of Batch Normalization.",
    "DCGANs": "Deep Convolutional Generative Adversarial Networks that use self-attention to stabilize training.",
    "High negative coefficient": "Indicates a strong negative relationship between a feature and the target variable in a linear regression model.",
    "Models' accuracy on CIFAR-10": "Some models achieve over 98% accuracy on the CIFAR-10 dataset as of 2020.",
    "Original ResNets optimization": "The original ResNets were not optimized using the Adam optimizer.",
    "K-means algorithm": "An iterative clustering algorithm that aims to partition n observations into k clusters.",
    "convolutional kernels": "Small matrices used for feature extraction in convolutional neural networks.",
    "Batch Normalization": "Technique to normalize activations in neural networks",
    "Density estimation": "A method to estimate the probability density function of a random variable.",
    "kernel density estimator": "A non-parametric way to estimate the probability density function of a random variable.",
    "classification": "The process of categorizing input data into classes or categories.",
    "correspondence": "A relationship or connection between two things.",
    "logistic regression": "A statistical model used for binary classification.",
    "Gaussian Naive Bayes": "A variant of the Naive Bayes algorithm that assumes Gaussian distribution of features.",
    "clustering": "The task of grouping a set of objects in such a way that objects in the same group are more similar to each other.",
    "weak classifier": "A classifier that performs slightly better than random guessing.",
    "MLE": "Maximum Likelihood Estimation, a method to estimate the parameters of a statistical model.",
    "Gradient descent": "An optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent.",
    "decision trees": "A predictive model that maps features to conclusions about the target value.",
    "linear regression": "A linear approach to modeling the relationship between a dependent variable and one or more independent variables.",
    "Neural networks": "A set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns.",
    "Boolean random variable": "A variable that can take on one of two values, typically true or false.",
    "TP": "Abbreviation for 'tests positive' in the context of medical testing.",
    "radial basis kernel function": "A kernel function that depends only on the distance between the input and a fixed point.",
    "VC dimension": "a measure of the capacity of a statistical classification algorithm, representing the complexity of the decision boundary it can represent.",
    "Perceptron": "A type of artificial neuron used for binary classification tasks",
    "SVM": "Support Vector Machine, a supervised learning model used for classification and regression analysis.",
    "Grid search": "A hyperparameter tuning technique that exhaustively searches through a specified subset of hyperparameters.",
    "regression": "A statistical method used to examine the relationship between two or more variables.",
    "pruning": "A technique in machine learning that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances.",
    "softmax function": "A function that converts a vector of real values into a vector of probabilities.",
    "entropy": "A measure of uncertainty or randomness in a set of outcomes.",
    "Bayesian Network": "A probabilistic graphical model representing a set of random variables and their conditional dependencies.",
    "ID3 algorithm": "a decision tree learning algorithm that uses entropy and information gain to construct decision trees.",
    "Neural Network": "A computational model inspired by the structure and function of the human brain, composed of interconnected nodes that mimic biological neurons.",
    "Boosting algorithm": "An ensemble learning method that combines multiple weak learners to create a strong learner, with each subsequent model correcting errors made by the previous ones.",
    "Entropy Loss": "A loss function used in machine learning to measure the difference between probability distributions, often used in classification tasks.",
    "Sigmoid Activation Function": "A type of activation function in neural networks that squashes the output to a range between 0 and 1, commonly used in binary classification problems.",
    "Global Optimum": "The best possible solution for a given optimization problem, representing the lowest possible value of the objective function across all possible solutions.",
    "Out-of-distribution detection": "A task in machine learning that involves identifying input samples that do not belong to any of the known classes or categories.",
    "Cross validation": "A resampling technique used to evaluate machine learning models by partitioning the data into subsets for training and testing to assess their performance.",
    "Highway Networks": "A type of neural network architecture that enables the training of very deep networks by incorporating gating mechanisms to control information flow.",
    "DenseNets": "Densely connected neural networks that establish direct connections between all layers, promoting feature reuse and alleviating the vanishing gradient problem.",
    "Nearest Neighbors": "A non-parametric classification algorithm that classifies new data points based on the majority class among their nearest neighbors in the training set.",
    "Feedforward Neural Network": "A type of neural network where connections between nodes do not form cycles, with information flowing in one direction from input to output layers.",
    "Self-attention": "A mechanism in neural networks that allows each element in a sequence to focus on different parts of the sequence, capturing long-range dependencies efficiently.",
    "ReLU": "an activation function that outputs the input directly if it is positive, otherwise, it outputs zero.",
    "Gradient Descent": "An optimization algorithm used to minimize the cost function by iteratively moving in the direction of steepest descent.",
    "Linearly Separable": "A property of data points that can be separated by a linear decision boundary, often in the context of classification tasks.",
    "Spatial Clustering Algorithms": "Algorithms used to group spatial data points based on their geographical proximity or spatial relationships, such as K-means or DBSCAN.",
    "L2 regularization": "a technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function based on the L2 norm of the weights.",
    "Residual Connections": "Connections in neural networks that skip one or more layers, allowing the gradient to flow directly through the network and alleviate the vanishing gradient problem.",
    "Conditional Independence": "When two random variables are independent of each other given the value of a third random variable.",
    "Bagging": "an ensemble learning technique that combines multiple models trained on different subsets of the data to reduce variance and prevent overfitting.",
    "PCA": "Principal Component Analysis, a dimensionality reduction technique that identifies the directions of maximum variance in a dataset.",
    "Spectral Clustering": "A clustering technique that uses eigenvalues and eigenvectors of a similarity matrix to partition data into clusters",
    "Logistic Regression": "A statistical model used for binary classification that predicts the probability of a binary outcome",
    "Linear Regression": "A linear approach to modeling the relationship between a dependent variable and one or more independent variables",
    "Stanford Sentiment Treebank": "A dataset containing movie reviews used for sentiment analysis tasks",
    "Penn Treebank": "A large annotated corpus of English text used for language modeling and other natural language processing tasks",
    "Null Space Dimensionality": "The number of linearly independent vectors in the null space of a matrix",
    "Support Vectors": "Data points that define the decision boundary between classes in a support vector machine",
    "Word2Vec": "A technique for learning word embeddings from a large corpus of text",
    "Restricted Boltzmann Machine": "A generative stochastic neural network used for dimensionality reduction and feature learning",
    "Activation Function": "A function that introduces nonlinearity into neural networks, enabling them to learn complex patterns",
    "Training Loss": "A measure of the error between predicted and actual values during the training phase of a machine learning model",
    "Posterior Probability": "The probability of an event occurring given additional evidence or information",
    "Independently and Identically Distributed": "Assumption in machine learning that training and test data come from the same distribution and are independent of each other",
    "COCO": "a large-scale object detection, segmentation, and captioning dataset",
    "ImageNet": "A large-scale dataset of images used for training and evaluating computer vision algorithms.",
    "Kernel": "A function that computes the dot product of data points in a higher-dimensional space without explicitly transforming them",
    "K-fold Cross-validation": "A technique for assessing the performance of a machine learning model by splitting data into k subsets",
    "ResNet-50": "A deep convolutional neural network architecture with 50 layers known for its residual connections",
    "Boolean Random Variables": "Random variables that can take on only two values, typically true or false.",
    "Decision Tree": "A tree-like model of decisions and their possible consequences, used for classification and regression tasks.",
    "Naive Bayes Classifier": "A simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions",
    "Gaussian Bayes Classifier": "A classifier that assumes the likelihood of the features given the class label follows a Gaussian distribution.",
    "Hypothesis Space": "The set of all possible functions or models that a machine learning algorithm can choose as the solution",
    "Overfitting": "When a machine learning model learns the detail and noise in the training data to the extent that it negatively impacts the performance on new data.",
    "EM": "Expectation-Maximization algorithm for estimating parameters in statistical models.",
    "Gaussian Mixture Model": "A probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions.",
    "Junction Tree Algorithm": "A method for performing exact inference in Bayesian networks by converting the network into a tree structure.",
    "Variable Elimination": "A method for computing marginal probabilities in Bayesian networks by eliminating variables in a specific order.",
    "D-Separation": "A criterion in Bayesian networks to determine whether two nodes are conditionally independent given a set of observed nodes.",
    "Cluster Analysis": "A machine learning task that involves grouping similar data points together based on certain criteria.",
    "Principal Component Analysis": "A dimensionality reduction technique that transforms data into a new coordinate system to find the directions of maximum variance.",
    "Singular Value Decomposition": "A matrix factorization method that decomposes a matrix into the product of orthogonal matrices and a diagonal matrix of singular values.",
    "MAP Estimate": "Maximum A Posteriori estimate, which finds the most probable value of a parameter given the observed data and a prior distribution.",
    "MLE Estimate": "Maximum Likelihood Estimate, which finds the value of a parameter that maximizes the likelihood of the observed data.",
    "Regularization": "A technique used to prevent overfitting by adding a penalty term to the model's loss function.",
    "Discriminative Models": "Machine learning models that directly model the decision boundary between classes without explicitly modeling the underlying probability distributions.",
    "Ensemble Learning": "A machine learning technique that combines multiple models to improve predictive performance.",
    "Bayesian vs. Frequentist": "The philosophical differences in interpreting probability and statistical inference between the two schools of thought.",
    "BLEU Metric": "A metric used to evaluate the quality of machine-translated text based on precision.",
    "ROGUE Metric": "A metric used to evaluate the quality of machine-generated text summaries based on recall.",
    "Hidden Markov Model": "A statistical model used to model sequences of observable events with underlying unobservable states.",
    "Caltech-101": "A dataset commonly used for object recognition tasks in computer vision.",
    "Feature Selection": "The process of selecting a subset of relevant features to use in model training and prediction.",
    "Maximum A Posteriori": "An estimation method that finds the most probable value of a parameter given the observed data and a prior distribution.",
    "Entropy Function": "A measure of uncertainty or disorder in a random variable or system.",
    "Supervised Learning": "A machine learning task where the model is trained on labeled data to make predictions or decisions.",
    "Neural Network Convergence": "The process by which a neural network's weights and biases adjust to minimize the loss function during training.",
    "Learning Rate": "A hyperparameter that controls the size of the update to the model weights during training.",
    "Dropout": "A regularization technique in neural networks where random units are set to zero during training to prevent overfitting.",
    "Clustering": "The task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.",
    "Regularization Penalty": "An additional term added to the loss function during training to prevent overfitting.",
    "Zeroed Coefficients": "When the coefficients of certain features in a model are set to zero, effectively removing them from the model.",
    "Penalties": "Additional terms added to the loss function during training to impose constraints on the model parameters.",
    "HMM": "a statistical model that assumes the system being modeled is a Markov process with hidden states.",
    "Collaborative filtering": "A method used to make automatic predictions about the interests of a user by collecting preferences from many users.",
    "Bias": "The error introduced by approximating a real-world problem, which can lead to underfitting.",
    "Variance": "The error introduced by approximating a real-world problem, which can lead to overfitting.",
    "Sigmoid": "An activation function that maps input values to probabilities between 0 and 1.",
    "Gradient": "A vector that contains the partial derivatives of a function with respect to each parameter.",
    "Convolution kernel": "A matrix used for the convolution operation in neural networks to extract features from the input data.",
    "False": "Not true or incorrect statement in the context of the discussion.",
    "Parameters": "The weights and biases in a neural network that are learned during the training process.",
    "convolutional networks": "a type of deep neural network commonly used for processing grid-like data, such as images",
    "graph networks": "neural networks designed to operate on graph-structured data",
    "fully connected networks": "neural networks where each neuron in one layer is connected to every neuron in the next layer",
    "RBF networks": "Radial Basis Function networks, which use radial basis functions as activation functions",
    "O(D)": "complexity of an algorithm that grows linearly with the size of the input data",
    "O(N)": "complexity of an algorithm that grows linearly with the number of elements in the input data",
    "O(ND)": "complexity of an algorithm that grows linearly with both the size of the input data and the number of elements",
    "O(ND^2)": "complexity of an algorithm that grows quadratically with the size of the input data and linearly with the number of elements",
    "Lower variance": "a situation where data points are closer to the mean, indicating less variability in the dataset",
    "Higher variance": "a situation where data points are spread out from the mean, indicating higher variability in the dataset",
    "good fitting": "a model that accurately captures the underlying patterns in the data without underfitting or overfitting",
    "overfitting": "When a machine learning model performs well on training data but poorly on unseen data.",
    "underfitting": "When a machine learning model is too simple to capture the underlying patterns in the data.",
    "feature space": "The space where the features of a dataset exist, with each dimension representing a different feature.",
    "objective function": "A function that is optimized or minimized during the process of training a machine learning model.",
    "within class variance": "The variance of data points within the same class or cluster in a clustering algorithm.",
    "global optimum": "The best possible solution for a given optimization problem across all possible solutions.",
    "Decision Trees": "A predictive modeling approach that uses a tree-like graph of decisions and their possible consequences.",
    "Density-based clustering": "A clustering method that groups together points that are closely packed in high-density regions.",
    "Model-based clustering": "Clustering approach that assumes data is generated from a mixture of probability distributions.",
    "K-means clustering": "An iterative clustering algorithm that aims to partition data points into K clusters based on mean values.",
    "bias": "Systematic error or deviation of the model's predictions from the true values, often due to simplifying assumptions.",
    "variance": "The variability or spread of model predictions around the true values, indicating sensitivity to fluctuations in the training data.",
    "Increase bias": "To intentionally introduce more error in predictions to reduce the model's sensitivity to noise in the training data.",
    "Best-subset selection": "A feature selection technique that evaluates all possible combinations of features to identify the best subset.",
    "convex objective function": "An objective function that is convex, meaning it has a single global minimum that optimization algorithms can converge to.",
    "Supervised learning": "A type of machine learning where the model is trained on labeled data, with the goal of predicting an output based on input data.",
    "Unsupervised learning": "A type of machine learning where the model is trained on unlabeled data, with the goal of discovering patterns or structures within the data.",
    "training set error": "The difference between the predicted outputs of a model and the actual outputs on the training data.",
    "optimisation algorithm": "A method used to minimize the error or loss function during the training of a machine learning model.",
    "linear SVM": "A machine learning algorithm that finds the optimal linear boundary to separate two classes in two-dimensional data points.",
    "Gaussian kernel SVM": "A support vector machine algorithm that uses a Gaussian kernel function to map data into a higher-dimensional space, allowing for nonlinear separation.",
    "closed form basis expansion": "A method to represent the solution of a problem using a linear combination of basis functions, providing a simpler mathematical form.",
    "Stochastic Gradient Descent": "An optimization algorithm that updates the model parameters using a random subset of training examples at each iteration.",
    "Mini-Batch Gradient Descent": "An optimization algorithm that updates the model parameters using a small random subset of training examples at each iteration.",
    "Batch Gradient Descent": "An optimization algorithm that updates the model parameters using all training examples at each iteration.",
    "model bias": "The error introduced by approximating a real-world problem, typically reduced by increasing model complexity.",
    "anomaly detection": "The task of identifying rare items, events, or observations which raise suspicions by differing significantly from the majority of the data.",
    "robustness": "The ability of a system to maintain its performance under variations in input, model parameters, or environmental conditions.",
    "O(1)": "Indicates that the time or space complexity of an algorithm remains constant regardless of the input size.",
    "unbounded": "Not limited or restricted within a specific range of values, extending infinitely.",
    "Linear hard-margin SVM": "A support vector machine algorithm that aims to find the optimal linear boundary with no misclassifications in the training data.",
    "Partitioning based clustering": "A clustering method that partitions data into distinct groups based on specified criteria or similarity measures.",
    "sampling with replacement": "A technique where each sample drawn from a dataset is placed back in the dataset before the next sample is drawn, allowing for duplicates.",
    "weak classifiers": "Simple classifiers that perform only slightly better than random chance, often used in ensemble methods to create strong classifiers.",
    "validation": "The process of evaluating a trained machine learning model on a separate dataset to assess its performance and generalization ability.",
    "decision boundary": "The boundary that separates different classes in a machine learning model.",
    "data centroid": "The center point of a dataset, calculated as the average of all data points.",
    "non-zero weight αk": "Data points in a Support Vector Machine that have a non-zero impact on the decision boundary.",
    "Step size": "The amount by which the model parameters are updated during each iteration of training.",
    "Expectation Maximization": "An iterative algorithm used to estimate the parameters of probabilistic models when some of the variables are unobserved.",
    "CART": "Classification and Regression Trees, a decision tree algorithm used for both classification and regression tasks.",
    "Gaussian Naïve Bayes": "A variant of the Naïve Bayes algorithm that assumes a Gaussian distribution for the features.",
    "Apriori": "An algorithm used for association rule mining in data analysis.",
    "learning rate": "A hyperparameter that controls the size of the updates to the model during training.",
    "linear in K": "A complexity measure indicating that the computational cost grows linearly with the size of the input data.",
    "quadratic in K": "A complexity measure indicating that the computational cost grows quadratically with the size of the input data.",
    "cubic in K": "A complexity measure indicating that the computational cost grows cubically with the size of the input data.",
    "exponential in K": "A complexity measure indicating that the computational cost grows exponentially with the size of the input data.",
    "Nando de Frietas": "A prominent figure in the field of machine learning and Bayesian deep learning.",
    "Yann LeCun": "A computer scientist known for his work in artificial intelligence and deep learning, particularly convolutional neural networks.",
    "Stuart Russell": "A renowned AI researcher and author known for his contributions to the field of artificial intelligence.",
    "Jitendra Malik": "A computer vision researcher known for his work on visual object recognition and scene understanding.",
    "kernel function": "A function that computes the similarity between pairs of data points in a higher-dimensional space.",
    "Euclidian": "A distance metric that calculates the straight-line distance between two points in Euclidean space.",
    "L1": "A distance metric that calculates the sum of the absolute differences between the components of two vectors.",
    "kernel width": "A parameter that determines the width of the kernel function in kernel methods.",
    "Maximum Likelihood": "A method of estimating the parameters of a statistical model by maximizing the likelihood function.",
    "class covariance matrices": "Matrices that describe the covariance between features within each class in a classification problem.",
    "diagonal class covariance matrices": "Covariance matrices in which off-diagonal elements are assumed to be zero, simplifying calculations.",
    "class priors": "The probabilities of each class in a classification problem.",
    "mean vectors": "Vectors that represent the average values of features within each class in a classification problem.",
    "Training error": "The error or difference between the predicted output of a machine learning model and the actual output on the training data.",
    "Testing error": "The error or difference between the predicted output of a machine learning model and the actual output on the testing data.",
    "Probabilistic regression": "A regression approach that models the uncertainty in the output predictions by using probabilistic methods.",
    "Prior distributions": "Probability distributions that represent beliefs or assumptions about the parameters of a model before observing the data.",
    "Gaussian Discriminant Analysis": "A classification algorithm that assumes the features follow a Gaussian distribution within each class.",
    "Ridge": "A regularization technique in linear regression that adds a penalty term to the cost function to prevent overfitting.",
    "Lasso": "A regularization technique in linear regression that adds a penalty term to the cost function to promote sparsity in the model.",
    "Class priors": "The prior probabilities of observing each class in a classification problem before seeing any data.",
    "Naive Bayesian": "A classification technique based on Bayes' theorem with the assumption of independence between features.",
    "L0 norm": "The norm that counts the number of non-zero elements in a vector or matrix.",
    "L1 norm": "The norm that represents the sum of the absolute values of the elements in a vector or matrix.",
    "L2 norm": "The norm that represents the Euclidean distance or magnitude of a vector or matrix.",
    "P(A|B)": "Conditional probability of A given B",
    "P(B|A)": "Conditional probability of B given A",
    "P(B)": "Probability of event B",
    "Weight initialization": "Process of setting initial weights in a neural network",
    "Convolving": "Applying a filter to an image using convolution operation",
    "Semantic segmentation": "Segmenting an image at pixel level based on classes",
    "IoU": "Measure of overlap between two bounding boxes",
    "Linear network": "Network without activation functions resulting in linear transformations",
    "Leaky ReLU": "Rectified Linear Unit with a small slope for negative values",
    "Convex": "Function with a non-negative curvature",
    "Concave": "Function with a non-positive curvature",
	"pure":"not mixed or adulterated with any other substance or material.",
	"feature":"a distinctive attribute or aspect of something.",
	"hidden nodes":"nodes have no direct connection with the outside world.",
	"polynomial":"one whose running time grows as a polynomial function of the size of its input.",
	"Forward stage wise selection":"adds features sequentially to maximize model performance.",
	"activation functions":"function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it.",
	"causal relationships":" a cause-and-effect relationship where one event or variable directly results in the occurrence of another event or change in another variable.",
	"Maximization":"step involves the use of estimated data in the E-step and updating the parameters."
}